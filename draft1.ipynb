{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1276317,"sourceType":"datasetVersion","datasetId":735911}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"8a6fab66-10a0-4d4b-ab07-b3b0bfb96e59","cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\nimport kagglehub\nshubhamgoel27_dermnet_path = kagglehub.dataset_download('shubhamgoel27/dermnet')\n\nprint('Data source import complete.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:05.773937Z","iopub.execute_input":"2025-05-04T06:09:05.774215Z","iopub.status.idle":"2025-05-04T06:09:06.058166Z","shell.execute_reply.started":"2025-05-04T06:09:05.774197Z","shell.execute_reply":"2025-05-04T06:09:06.057595Z"}},"outputs":[{"name":"stdout","text":"Data source import complete.\n","output_type":"stream"}],"execution_count":1},{"id":"e6be3109-695b-4bee-af70-b810297e6df9","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.transforms import v2\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nimport kagglehub\nimport os\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom torch.utils.data import SubsetRandomSampler\nfrom torch.utils.data import DataLoader\n\nfrom safetensors.torch import save_file, load_file\n#from kaggle_secrets import UserSecretsClient\nimport wandb\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom torchvision.models import efficientnet_b3  # EfficientNet\nfrom safetensors.torch import save_file, load_file\nfrom collections import Counter  # Added\nimport json\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:06.061592Z","iopub.execute_input":"2025-05-04T06:09:06.061921Z","iopub.status.idle":"2025-05-04T06:09:09.946048Z","shell.execute_reply.started":"2025-05-04T06:09:06.061894Z","shell.execute_reply":"2025-05-04T06:09:09.945489Z"}},"outputs":[],"execution_count":2},{"id":"7623bcb4-ef02-49f5-83ee-80534eb6b0e6","cell_type":"code","source":"\nimport os\nos.environ['WANDB_NOTEBOOK_NAME'] = 'dl.ipynb' \nstatus = wandb.login(key='78fdef8c508ef3b1b92f66345a383b486d6b36f3')             #changed key, reset its\nif (status):\n     print('Successfully logged into W&B')\nelse:\n     print('Unable to log into W&B')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:09.946942Z","iopub.execute_input":"2025-05-04T06:09:09.947156Z","iopub.status.idle":"2025-05-04T06:09:10.011774Z","shell.execute_reply.started":"2025-05-04T06:09:09.947138Z","shell.execute_reply":"2025-05-04T06:09:10.011079Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find dl.ipynb.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkehyiqian\u001b[0m (\u001b[33mkehyiqian-universiti-tunku-abdul-rahman\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"Successfully logged into W&B\n","output_type":"stream"}],"execution_count":3},{"id":"a06d36db-f82f-42ea-9634-f51d7ba4ed37","cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:10.013472Z","iopub.execute_input":"2025-05-04T06:09:10.013669Z","iopub.status.idle":"2025-05-04T06:09:10.073798Z","shell.execute_reply.started":"2025-05-04T06:09:10.013652Z","shell.execute_reply":"2025-05-04T06:09:10.073078Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":4},{"id":"690f5270-9bbe-4799-a501-f0ba4785665e","cell_type":"code","source":"def save_model_and_history(model, history, save_dir=\"checkpoints\", model_name=\"model\", epoch=None):\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Save model weights\n    model_path = os.path.join(save_dir, f\"{model_name}_weights.pth\")\n    torch.save(model.state_dict(), model_path)\n\n    # Save training history\n    history_path = os.path.join(save_dir, f\"{model_name}_history.json\")\n    with open(history_path, \"w\") as f:\n        json.dump(history, f)\n\n    print(f\"Model saved to: {model_path}\")\n    print(f\"History saved to: {history_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:10.074454Z","iopub.execute_input":"2025-05-04T06:09:10.074634Z","iopub.status.idle":"2025-05-04T06:09:10.081523Z","shell.execute_reply.started":"2025-05-04T06:09:10.074620Z","shell.execute_reply":"2025-05-04T06:09:10.080815Z"}},"outputs":[],"execution_count":5},{"id":"2f6840e6-b69f-41cf-8f81-0d8037311932","cell_type":"code","source":"def train_one_epoch(epoch, model, train_loader, criterion, optimizer, device=\"cuda\", log_step=20, mixup_alpha=0.1):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for i, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Mixup\n        if mixup_alpha > 0:\n            lam = np.random.beta(mixup_alpha, mixup_alpha)\n            rand_index = torch.randperm(inputs.size(0)).to(device)\n            inputs = lam * inputs + (1 - lam) * inputs[rand_index]\n            labels_a, labels_b = labels, labels[rand_index]\n        else:\n            labels_a = labels_b = labels\n            lam = 1.0\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n\n        # For metrics\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        correct += (lam * predicted.eq(labels_a).sum().item() + (1 - lam) * predicted.eq(labels_b).sum().item())\n        total += labels.size(0)\n\n        if i % log_step == 0 or i == len(train_loader) - 1:\n            print(f\"[Epoch {epoch+1}, Step {i+1}] train_loss: {running_loss / (i + 1):.4f}\")\n\n    train_loss = running_loss / len(train_loader)\n    train_acc = 100 * correct / total\n    return train_loss, train_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:10.082260Z","iopub.execute_input":"2025-05-04T06:09:10.082491Z","iopub.status.idle":"2025-05-04T06:09:10.095547Z","shell.execute_reply.started":"2025-05-04T06:09:10.082466Z","shell.execute_reply":"2025-05-04T06:09:10.094917Z"}},"outputs":[],"execution_count":6},{"id":"f18f8273-1a2f-48dc-a497-2f40adbbe7c3","cell_type":"code","source":"def evaluate(model, test_loader, criterion, device=\"cuda\"):\n    model.eval()\n    correct = 0\n    total = 0\n    test_loss = 0.0\n    with torch.inference_mode():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    test_loss += loss.item() * inputs.size(0)  # scale loss by batch size\n    test_loss /= total\n    test_acc = 100 * correct / total\n\n    return test_loss, test_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:10.096255Z","iopub.execute_input":"2025-05-04T06:09:10.096502Z","iopub.status.idle":"2025-05-04T06:09:10.110875Z","shell.execute_reply.started":"2025-05-04T06:09:10.096484Z","shell.execute_reply":"2025-05-04T06:09:10.110112Z"}},"outputs":[],"execution_count":7},{"id":"4d83277e-be87-49d4-ae81-1e8108b7772d","cell_type":"code","source":"#Address Class Imbalance #Focal Loss will focus on hard examples, particularly minority classes, improving overall Test Accuracy. #added label smoothing\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0, reduction='mean', label_smoothing=0.1):   #high gamma may over-focus on hard examples, causing fluctuations.smoothen testloss and generalisation\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.reduction = reduction\n        self.alpha = alpha\n        self.label_smoothing = label_smoothing\n\n    def forward(self, inputs, targets):\n        ce_loss = nn.CrossEntropyLoss(weight=self.alpha, reduction='none', label_smoothing=self.label_smoothing)(inputs, targets)\n        pt = torch.exp(-ce_loss)\n        focal_loss = (1 - pt) ** self.gamma * ce_loss\n\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        return focal_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:10.111614Z","iopub.execute_input":"2025-05-04T06:09:10.111851Z","iopub.status.idle":"2025-05-04T06:09:10.125111Z","shell.execute_reply.started":"2025-05-04T06:09:10.111833Z","shell.execute_reply":"2025-05-04T06:09:10.124525Z"}},"outputs":[],"execution_count":8},{"id":"0afa2957-cd87-47cb-8ebf-794da39c37f4","cell_type":"code","source":"from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau\n\ndef train(model, optimizer, criterion, train_loader, test_loader, config, device=\"cuda\", model_name=\"model\"):\n\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n   \n    best_acc = 0\n    patience = 7\n    patience_counter = 0\n\n    history = {\n        \"train_loss\": [],\n        \"train_acc\": [],\n        \"val_loss\": [],\n        \"val_acc\": []\n    }\n\n    for epoch in range(config['num_epochs']):\n        train_loss, train_acc = train_one_epoch(epoch, model, train_loader, criterion, optimizer, device)\n        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(test_loss)\n        history['val_acc'].append(test_acc)\n\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"validation_loss\": test_loss,\n            \"validation_acc\": test_acc,\n            \"lr_backbone\": optimizer.param_groups[0]['lr'],\n            \"lr_classifier\": optimizer.param_groups[1]['lr']\n        })\n\n\n        print(f\"[Epoch {epoch+1}] Train Acc: {train_acc:.2f}%, Train Loss: {train_loss:.4f}, Validation Acc: {test_acc:.2f}%, Validation Loss: {test_loss:.4f}\")\n\n        # Save best model\n        if test_acc > best_acc:\n            best_acc = test_acc\n            patience_counter = 0\n            save_file(model.state_dict(), f\"best_model_epoch_{epoch+1}.safetensors\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n        scheduler.step(test_loss)\n\n    save_model_and_history(model, history, model_name=model_name)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:10.125806Z","iopub.execute_input":"2025-05-04T06:09:10.126027Z","iopub.status.idle":"2025-05-04T06:09:10.136443Z","shell.execute_reply.started":"2025-05-04T06:09:10.126012Z","shell.execute_reply":"2025-05-04T06:09:10.135928Z"}},"outputs":[],"execution_count":9},{"id":"e716789b-4507-4ec4-9ec7-d8620831333e","cell_type":"code","source":"from torchvision.models import (\n    densenet121, mobilenet_v2, efficientnet_b3, DenseNet121_Weights, MobileNet_V2_Weights, EfficientNet_B3_Weights\n)\nimport torch.nn as nn\nimport torch.optim as optim\n\ndef build_model(model_name, num_classes, config, device='cuda', return_optimizer=True):\n    # Select model\n    if model_name == 'densenet':\n        model = densenet121(weights=config['weights']).to(device)\n        in_features = model.classifier.in_features\n        model.classifier = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        ).to(device)\n    elif model_name == 'mobilenet':\n        model = mobilenet_v2(weights=config['weights']).to(device)\n        in_features = model.classifier[1].in_features\n        model.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(in_features, num_classes)\n        ).to(device)\n    elif model_name == 'efficientnet':\n        model = efficientnet_b3(weights=config['weights']).to(device)\n        in_features = model.classifier[1].in_features\n        model.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(in_features, num_classes)\n        ).to(device)\n    else:\n        raise ValueError(\"Unsupported model name\")\n\n    # Finetuning strategy\n    finetuned_layers = config.get('finetuned_layers', 'all')\n\n    if finetuned_layers == 'classifier':\n        for param in model.parameters():\n            param.requires_grad = False\n        for param in model.classifier.parameters():\n            param.requires_grad = True\n    elif finetuned_layers == 'features_last' and model_name == 'efficientnet':\n        for param in model.parameters():\n            param.requires_grad = False\n        for param in model.features[-1].parameters():\n            param.requires_grad = True\n        for param in model.classifier.parameters():\n            param.requires_grad = True\n    elif finetuned_layers == 'all':\n        for param in model.parameters():\n            param.requires_grad = True\n    else:\n        for name, param in model.named_parameters():\n            if not any(name.startswith(layer) for layer in config['finetuned_layers']):\n                param.requires_grad = False\n\n    # Print model info\n    print(f\"Model           : {model_name}\")\n    print(f\"Weights         : {config['weights']}\")\n    print(f\"Finetuned layers: {finetuned_layers}\\n\")\n\n    # Optimizer\n    if not return_optimizer:\n        return model\n\n    if hasattr(model, 'features') and hasattr(model, 'classifier'):\n        optimizer = torch.optim.AdamW([\n            {\"params\": model.features.parameters(), \"lr\": config.get(\"backbone_lr\",config['lr_features'])},\n            {\"params\": model.classifier.parameters(), \"lr\": config.get(\"classifier_lr\",config['lr_classifier'])}\n        ], weight_decay=config.get(\"weight_decay\", 1e-3))\n    else:\n        raise ValueError(\"Model must have 'features' and 'classifier' attributes.\")\n\n  \n\n    return model, optimizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:10.136991Z","iopub.execute_input":"2025-05-04T06:09:10.137196Z","iopub.status.idle":"2025-05-04T06:09:10.150496Z","shell.execute_reply.started":"2025-05-04T06:09:10.137182Z","shell.execute_reply":"2025-05-04T06:09:10.149743Z"}},"outputs":[],"execution_count":10},{"id":"ddfb0432-c3e4-4b9c-8522-b5bf60be910b","cell_type":"code","source":"path = kagglehub.dataset_download(\"shubhamgoel27/dermnet\") # directly download the dataset from kaggle\ndataset_path = os.listdir(path)\n\ntrain_path = os.path.join(path, 'train')   #train folder\ntest_path = os.path.join(path, 'test')     #test folder\n# List class names from the 'train' folder\nclass_names = [folder for folder in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, folder))]\nnum_classes = len(class_names)\nprint(f'Class names found in the training set: \\n{class_names} \\n')\nprint(f'{num_classes = } \\n')\n\nclass_names = [folder for folder in os.listdir(test_path) if os.path.isdir(os.path.join(test_path, folder))]\nnum_classes = len(class_names)\nprint(f'Class names found in the testing set: \\n{class_names} \\n')\nprint(f'{num_classes = } \\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:10.151345Z","iopub.execute_input":"2025-05-04T06:09:10.151620Z","iopub.status.idle":"2025-05-04T06:09:10.400386Z","shell.execute_reply.started":"2025-05-04T06:09:10.151586Z","shell.execute_reply":"2025-05-04T06:09:10.399841Z"}},"outputs":[{"name":"stdout","text":"Class names found in the training set: \n['Light Diseases and Disorders of Pigmentation', 'Lupus and other Connective Tissue diseases', 'Acne and Rosacea Photos', 'Systemic Disease', 'Poison Ivy Photos and other Contact Dermatitis', 'Vascular Tumors', 'Urticaria Hives', 'Atopic Dermatitis Photos', 'Bullous Disease Photos', 'Hair Loss Photos Alopecia and other Hair Diseases', 'Tinea Ringworm Candidiasis and other Fungal Infections', 'Psoriasis pictures Lichen Planus and related diseases', 'Melanoma Skin Cancer Nevi and Moles', 'Nail Fungus and other Nail Disease', 'Scabies Lyme Disease and other Infestations and Bites', 'Eczema Photos', 'Exanthems and Drug Eruptions', 'Herpes HPV and other STDs Photos', 'Seborrheic Keratoses and other Benign Tumors', 'Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions', 'Vasculitis Photos', 'Cellulitis Impetigo and other Bacterial Infections', 'Warts Molluscum and other Viral Infections'] \n\nnum_classes = 23 \n\nClass names found in the testing set: \n['Light Diseases and Disorders of Pigmentation', 'Lupus and other Connective Tissue diseases', 'Acne and Rosacea Photos', 'Systemic Disease', 'Poison Ivy Photos and other Contact Dermatitis', 'Vascular Tumors', 'Urticaria Hives', 'Atopic Dermatitis Photos', 'Bullous Disease Photos', 'Hair Loss Photos Alopecia and other Hair Diseases', 'Tinea Ringworm Candidiasis and other Fungal Infections', 'Psoriasis pictures Lichen Planus and related diseases', 'Melanoma Skin Cancer Nevi and Moles', 'Nail Fungus and other Nail Disease', 'Scabies Lyme Disease and other Infestations and Bites', 'Eczema Photos', 'Exanthems and Drug Eruptions', 'Herpes HPV and other STDs Photos', 'Seborrheic Keratoses and other Benign Tumors', 'Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions', 'Vasculitis Photos', 'Cellulitis Impetigo and other Bacterial Infections', 'Warts Molluscum and other Viral Infections'] \n\nnum_classes = 23 \n\n","output_type":"stream"}],"execution_count":11},{"id":"9f0d6c61-d65b-4967-9578-365d623198da","cell_type":"code","source":"class SkinDiseaseDataset(Dataset):\n    def __init__(self, path, transform=None, selected_classes=None):\n        self.path = path\n        self.transform = transform\n        self.classes = []            # filtered class folder names\n        self.class_to_idx = {}       # mapping: class name -> label index\n        self.images_path = []\n        self.targets = []\n\n        for i, cls in enumerate(sorted(os.listdir(path))):\n            if selected_classes is not None and cls not in selected_classes:\n                continue\n            self.classes.append(cls)\n            self.class_to_idx[cls] = len(self.class_to_idx)  # re-index\n            cls_path = os.path.join(path, cls)\n            for filename in os.listdir(cls_path):\n                file_path = os.path.join(cls_path, filename)\n                self.images_path.append(file_path)\n                self.targets.append(self.class_to_idx[cls])\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, idx):\n        image_path = self.images_path[idx]\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        label = self.targets[idx]\n        return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:10.400999Z","iopub.execute_input":"2025-05-04T06:09:10.401263Z","iopub.status.idle":"2025-05-04T06:09:10.407204Z","shell.execute_reply.started":"2025-05-04T06:09:10.401246Z","shell.execute_reply":"2025-05-04T06:09:10.406462Z"}},"outputs":[],"execution_count":12},{"id":"3b59c640-c329-423a-82f6-0d1d1b78b1e8","cell_type":"code","source":"\ntransform_train = v2.Compose([\n    v2.ToImage(), # Convert input to PIL Image if needed\n    v2.Resize((224,224)), # Resize to 512x512, 224x224\n    v2.RandomHorizontalFlip(), # Randomly flip horizontally\n    v2.RandomVerticalFlip(p=0.5),  # Added vertical flip\n    v2.RandomRotation(degrees=10),                                                                # Reduced to 5 degrees from 10\n\n    \n    # v2.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Soften translation and scaling\n    v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Soften color jitter\n    # v2.RandomErasing(p=0.3, scale=(0.02, 0.1), ratio=(0.3, 3.3)),  # Soften random erasing\n    v2.ToDtype(torch.float32, scale=True), # Scale to [0,1] and make float32\n    v2.Normalize(mean=[0.485, 0.456, 0.406],  std=[0.229, 0.224, 0.225]),# Normalization\n])\n\ntransform_test = v2.Compose([\n    v2.ToImage(),                         \n    v2.Resize((224,224)),                 \n    v2.ToDtype(torch.float32, scale=True), \n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:10.410525Z","iopub.execute_input":"2025-05-04T06:09:10.410752Z","iopub.status.idle":"2025-05-04T06:09:10.424452Z","shell.execute_reply.started":"2025-05-04T06:09:10.410730Z","shell.execute_reply":"2025-05-04T06:09:10.423789Z"}},"outputs":[],"execution_count":13},{"id":"7b7111ca-d4d9-4caa-8b1f-da77cf07201b","cell_type":"code","source":"train_dataset = SkinDiseaseDataset(train_path, transform_train)\ntest_dataset = SkinDiseaseDataset(test_path, transform_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:10.425117Z","iopub.execute_input":"2025-05-04T06:09:10.425418Z","iopub.status.idle":"2025-05-04T06:09:10.516759Z","shell.execute_reply.started":"2025-05-04T06:09:10.425396Z","shell.execute_reply":"2025-05-04T06:09:10.516056Z"}},"outputs":[],"execution_count":14},{"id":"791ae13f-4700-46f4-bf78-3761ce0a2ba3","cell_type":"code","source":"#15\n# use class weights to distru\n# Compute class weights\ndef check_class_distribution(dataset):\n    labels = [label for _, label in dataset]\n    class_counts = Counter(labels)\n    print(\"Class distribution:\")\n    for class_idx, count in class_counts.items():\n        class_name = class_names[class_idx]\n        print(f\"{class_name}: {count} samples\")\n    return class_counts\n\nclass_counts = check_class_distribution(train_dataset)\ntotal_samples = sum(class_counts.values())\nclass_weights = torch.tensor([total_samples / (num_classes * class_counts[i]) for i in range(num_classes)], dtype=torch.float32)\nsmoothing_factor = 0.5\nuniform_weights = torch.ones(num_classes, dtype=torch.float32)\nclass_weights = smoothing_factor * class_weights + (1 - smoothing_factor) * uniform_weights         #Label smoothing can prevent the model from becoming overconfident, improving generalization.\nclass_weights = class_weights.to(device)\n\n\n# Update loss function with class weights\nloss_function = FocalLoss(alpha=class_weights, gamma=3.0, reduction='mean', label_smoothing=0.1)   #label smoothing\n# loss_function = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:09:10.517457Z","iopub.execute_input":"2025-05-04T06:09:10.517618Z","iopub.status.idle":"2025-05-04T06:12:09.198660Z","shell.execute_reply.started":"2025-05-04T06:09:10.517605Z","shell.execute_reply":"2025-05-04T06:12:09.197745Z"}},"outputs":[{"name":"stdout","text":"Class distribution:\nLight Diseases and Disorders of Pigmentation: 840 samples\nLupus and other Connective Tissue diseases: 1149 samples\nAcne and Rosacea Photos: 489 samples\nSystemic Disease: 448 samples\nPoison Ivy Photos and other Contact Dermatitis: 288 samples\nVascular Tumors: 1235 samples\nUrticaria Hives: 404 samples\nAtopic Dermatitis Photos: 239 samples\nBullous Disease Photos: 405 samples\nHair Loss Photos Alopecia and other Hair Diseases: 568 samples\nTinea Ringworm Candidiasis and other Fungal Infections: 420 samples\nPsoriasis pictures Lichen Planus and related diseases: 463 samples\nMelanoma Skin Cancer Nevi and Moles: 1040 samples\nNail Fungus and other Nail Disease: 260 samples\nScabies Lyme Disease and other Infestations and Bites: 1405 samples\nEczema Photos: 431 samples\nExanthems and Drug Eruptions: 1371 samples\nHerpes HPV and other STDs Photos: 606 samples\nSeborrheic Keratoses and other Benign Tumors: 1300 samples\nActinic Keratosis Basal Cell Carcinoma and other Malignant Lesions: 212 samples\nVasculitis Photos: 482 samples\nCellulitis Impetigo and other Bacterial Infections: 416 samples\nWarts Molluscum and other Viral Infections: 1086 samples\n","output_type":"stream"}],"execution_count":15},{"id":"c128b452-988e-46c1-800e-69cda55c08b6","cell_type":"code","source":"#16\nsplitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_idx, val_idx = next(splitter.split(train_dataset.images_path, train_dataset.targets))\n\nprint('Number of training samples:', len(train_idx), \"| indices:\", train_idx)\nprint('Number of val samples     :', len(val_idx), \"| indices:\", val_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:12:09.199545Z","iopub.execute_input":"2025-05-04T06:12:09.199765Z","iopub.status.idle":"2025-05-04T06:12:09.212676Z","shell.execute_reply.started":"2025-05-04T06:12:09.199747Z","shell.execute_reply":"2025-05-04T06:12:09.211865Z"}},"outputs":[{"name":"stdout","text":"Number of training samples: 12445 | indices: [ 8880 14613  9269 ... 14918  1808  2715]\nNumber of val samples     : 3112 | indices: [  474 14781   392 ...  8378 10909  8603]\n","output_type":"stream"}],"execution_count":16},{"id":"6599cbd6-f377-488d-89a8-c05ec47d56cb","cell_type":"code","source":"train_sampler = SubsetRandomSampler(train_idx)\nval_sampler = SubsetRandomSampler(val_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:12:09.213517Z","iopub.execute_input":"2025-05-04T06:12:09.213924Z","iopub.status.idle":"2025-05-04T06:12:09.217537Z","shell.execute_reply.started":"2025-05-04T06:12:09.213901Z","shell.execute_reply":"2025-05-04T06:12:09.216808Z"}},"outputs":[],"execution_count":17},{"id":"98be3d6f-261a-48fa-958e-3018fb7bc3b8","cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\nval_loader = DataLoader(train_dataset, batch_size=32, sampler=val_sampler)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:12:09.218232Z","iopub.execute_input":"2025-05-04T06:12:09.218433Z","iopub.status.idle":"2025-05-04T06:12:09.229376Z","shell.execute_reply.started":"2025-05-04T06:12:09.218419Z","shell.execute_reply":"2025-05-04T06:12:09.228790Z"}},"outputs":[],"execution_count":18},{"id":"280e9842-a94d-4696-917d-38f563c55535","cell_type":"code","source":"#19\nx_batch, y_batch = next(iter(train_loader))\nprint(f'{x_batch.shape = }')\nprint(f'{y_batch.shape = }')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:12:09.230011Z","iopub.execute_input":"2025-05-04T06:12:09.230272Z","iopub.status.idle":"2025-05-04T06:12:09.645091Z","shell.execute_reply.started":"2025-05-04T06:12:09.230250Z","shell.execute_reply":"2025-05-04T06:12:09.644457Z"}},"outputs":[{"name":"stdout","text":"x_batch.shape = torch.Size([32, 3, 224, 224])\ny_batch.shape = torch.Size([32])\n","output_type":"stream"}],"execution_count":19},{"id":"8a3a2802-4de6-48b2-85df-1c676bdb9308","cell_type":"markdown","source":"# densenet","metadata":{}},{"id":"951fbf75-80fc-4ed9-9250-ff210eeda2a8","cell_type":"code","source":"'''\nmodel_type = 'densenet'\nconfig =  {\n    'weights'         : 'IMAGENET1K_V1',\n    'finetuned_layers': 'all',\n    'lr_features'     : 1e-4,\n    'lr_classifier'   : 5e-4,\n    'num_epochs'      : 30\n}\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:12:09.645813Z","iopub.execute_input":"2025-05-04T06:12:09.646017Z","iopub.status.idle":"2025-05-04T06:12:09.651067Z","shell.execute_reply.started":"2025-05-04T06:12:09.646001Z","shell.execute_reply":"2025-05-04T06:12:09.650362Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"\"\\nmodel_type = 'densenet'\\nconfig =  {\\n    'weights'         : 'IMAGENET1K_V1',\\n    'finetuned_layers': 'all',\\n    'lr_features'     : 1e-4,\\n    'lr_classifier'   : 5e-4,\\n    'num_epochs'      : 30\\n}\\n\""},"metadata":{}}],"execution_count":20},{"id":"99a5cc94-36ac-4e95-9aa0-9960ef63bfb8","cell_type":"code","source":"#wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:12:09.651846Z","iopub.execute_input":"2025-05-04T06:12:09.652103Z","iopub.status.idle":"2025-05-04T06:12:09.665469Z","shell.execute_reply.started":"2025-05-04T06:12:09.652081Z","shell.execute_reply":"2025-05-04T06:12:09.664839Z"}},"outputs":[],"execution_count":21},{"id":"c3ab1098-36b5-4b22-b77f-334053344229","cell_type":"code","source":"'''\nwandb.init(\n    project='DL_groupAssigment',\n    name='All - output-test1',\n    config =  {\n    'weights'         : 'IMAGENET1K_V1',\n    'finetuned_layers': 'all',\n    'lr_features'     : 1e-4,\n    'lr_classifier'   : 5e-4,\n    'num_epochs'      : 30\n    },\n)\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:12:09.666085Z","iopub.execute_input":"2025-05-04T06:12:09.666727Z","iopub.status.idle":"2025-05-04T06:12:09.679522Z","shell.execute_reply.started":"2025-05-04T06:12:09.666699Z","shell.execute_reply":"2025-05-04T06:12:09.678936Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"\"\\nwandb.init(\\n    project='DL_groupAssigment',\\n    name='All - output-test1',\\n    config =  {\\n    'weights'         : 'IMAGENET1K_V1',\\n    'finetuned_layers': 'all',\\n    'lr_features'     : 1e-4,\\n    'lr_classifier'   : 5e-4,\\n    'num_epochs'      : 30\\n    },\\n)\\n\\n\""},"metadata":{}}],"execution_count":22},{"id":"2399b0bd-9284-4ee9-b3ff-749cf3b17da4","cell_type":"markdown","source":"# **DENSENET **","metadata":{}},{"id":"330cd774-7bc1-4843-9e1c-bf059b740edb","cell_type":"code","source":"#model_densenet, optimizer = build_model(model_type, num_classes, config, device)\n#train(model_densenet, optimizer, loss_function, train_loader, val_loader, config, device, model_name = \"model_densenet\")","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:12:09.680274Z","iopub.execute_input":"2025-05-04T06:12:09.680529Z","iopub.status.idle":"2025-05-04T06:12:09.689609Z","shell.execute_reply.started":"2025-05-04T06:12:09.680508Z","shell.execute_reply":"2025-05-04T06:12:09.688940Z"}},"outputs":[],"execution_count":23},{"id":"5dc99477-4f25-49f8-ad75-d63122389b35","cell_type":"code","source":"#cell 23","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:12:09.690363Z","iopub.execute_input":"2025-05-04T06:12:09.690575Z","iopub.status.idle":"2025-05-04T06:12:09.702143Z","shell.execute_reply.started":"2025-05-04T06:12:09.690561Z","shell.execute_reply":"2025-05-04T06:12:09.701619Z"}},"outputs":[],"execution_count":24},{"id":"148eaadf-6c73-483d-9516-15b14395c196","cell_type":"markdown","source":"# **MOBILENET **","metadata":{}},{"id":"578886fd-72fd-4965-9370-0907cc9c3c16","cell_type":"code","source":"# model_type = 'mobilenet'\n# config =  {\n#     'weights'         : 'IMAGENET1K_V1',\n#     'finetuned_layers': 'all',\n#     'lr'              : 3e-4,\n#     'num_epochs'      : 30\n# }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:12:09.702777Z","iopub.execute_input":"2025-05-04T06:12:09.702962Z","iopub.status.idle":"2025-05-04T06:12:09.716393Z","shell.execute_reply.started":"2025-05-04T06:12:09.702947Z","shell.execute_reply":"2025-05-04T06:12:09.715694Z"}},"outputs":[],"execution_count":25},{"id":"78a26652-22d5-465b-9d09-838698fd0387","cell_type":"code","source":"# model_mobilenet, optimizer = build_model(model_type, num_classes, config, device)\n# train(model_mobilenet, optimizer, loss_function, train_loader, val_loader, config, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:12:09.716982Z","iopub.execute_input":"2025-05-04T06:12:09.717217Z","iopub.status.idle":"2025-05-04T06:12:09.727865Z","shell.execute_reply.started":"2025-05-04T06:12:09.717198Z","shell.execute_reply":"2025-05-04T06:12:09.727265Z"}},"outputs":[],"execution_count":26},{"id":"77534fae-791b-491b-b348-cba60f1f4013","cell_type":"markdown","source":"# **EfficientNet **","metadata":{}},{"id":"29f65cfb-8f34-47a8-81cb-1f191aa5f7a1","cell_type":"code","source":"val_dataset = SkinDiseaseDataset(train_path, transform_test)\nval_loader = DataLoader(val_dataset, batch_size=32, sampler=val_sampler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:12:09.728513Z","iopub.execute_input":"2025-05-04T06:12:09.728723Z","iopub.status.idle":"2025-05-04T06:12:09.768289Z","shell.execute_reply.started":"2025-05-04T06:12:09.728705Z","shell.execute_reply":"2025-05-04T06:12:09.767826Z"}},"outputs":[],"execution_count":27},{"id":"f8717215-4735-4067-9ed9-b495a27580aa","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"369adf3f-8dd8-41a9-a55f-d3ec06973404","cell_type":"code","source":"# EfficientNet Configuration and Training\nconfig_efficientnet = {\n    'num_epochs': 6,\n    'weights': 'IMAGENET1K_V1',\n    'finetuned_layers': 'classifier',\n    'batch_size': 32,\n    'lr_features': 0.0001,  \n    'lr_classifier': 0.0005,  \n}\n\n# Training function for EfficientNet\ndef run_efficientnet(config, train_loader, val_loader, num_classes, device):\n    print(\"\\nTraining EfficientNet\")\n    model_type = 'efficientnet'\n\n    # Initialize WandB\n    wandb.init(\n        project='DL_groupAssigment',\n        name='EfficientNet_Training',\n        config=config\n    )\n\n    \n    # Stage 1: Fine-tune classifier\n    config['finetuned_layers'] = 'classifier'\n    config['num_epochs'] = 6\n    print(\"Stage 1: Fine-tuning classifier\")\n    model, optimizer = build_model(model_type, num_classes, config, device)\n    train(model, optimizer, loss_function, train_loader, val_loader, config, device, model_name=\"efficientnet\")\n\n    # Log Stage 1 results\n    val_loss, val_acc = evaluate(model, val_loader, loss_function, device)\n    wandb.log({\"stage\": \"Stage 1\", \"val_loss\": val_loss, \"val_acc\": val_acc})\n\n    # Clear GPU memory\n    torch.cuda.empty_cache()\n\n    # Stage 2: Fine-tune all layers\n    print(\"Stage 2: Fine-tuning all layers\")\n    config['finetuned_layers'] = 'all'\n    config['num_epochs'] = 24  # Total 30 epochs\n    config['lr_features'] = 1e-4\n    config['lr_classifier'] = 0.0001\n\n    for param in model.parameters():\n        param.requires_grad = True\n    \n    # Reinitialize optimizer with new learning rates\n    optimizer = torch.optim.AdamW([\n        {\"params\": model.features.parameters(), \"lr\": config['lr_features']},\n        {\"params\": model.classifier.parameters(), \"lr\": config['lr_classifier']}\n    ], weight_decay=config.get(\"weight_decay\", 1e-3))\n\n    train(model, optimizer, loss_function, train_loader, val_loader, config, device, model_name=\"efficientnet\")\n\n    # Log Stage 2 results\n    test_loss, test_acc = evaluate(model, test_loader, loss_function, device)\n    wandb.log({\"stage\": \"Stage 2\", \"test_loss\": test_loss, \"test_acc\": test_acc})\n    print(f\"EfficientNet Test Loss: {test_loss:.3f}, Test Accuracy: {test_acc:.2f}%\")\n    save_file(model.state_dict(), \"efficientnet_b3_all_lr0.0001.safetensors\")\n    print(\"Model saved.\")\n\n    return model\n\n\n\n# Run EfficientNet\nmodel_efficientnet = run_efficientnet(config_efficientnet, train_loader, val_loader, num_classes, device)\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T06:12:09.768875Z","iopub.execute_input":"2025-05-04T06:12:09.769059Z","iopub.status.idle":"2025-05-04T08:33:00.789448Z","shell.execute_reply.started":"2025-05-04T06:12:09.769045Z","shell.execute_reply":"2025-05-04T08:33:00.788601Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"name":"stdout","text":"\nTraining EfficientNet\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250504_061209-jklsao3v</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/kehyiqian-universiti-tunku-abdul-rahman/DL_groupAssigment/runs/jklsao3v' target=\"_blank\">EfficientNet_Training</a></strong> to <a href='https://wandb.ai/kehyiqian-universiti-tunku-abdul-rahman/DL_groupAssigment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/kehyiqian-universiti-tunku-abdul-rahman/DL_groupAssigment' target=\"_blank\">https://wandb.ai/kehyiqian-universiti-tunku-abdul-rahman/DL_groupAssigment</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/kehyiqian-universiti-tunku-abdul-rahman/DL_groupAssigment/runs/jklsao3v' target=\"_blank\">https://wandb.ai/kehyiqian-universiti-tunku-abdul-rahman/DL_groupAssigment/runs/jklsao3v</a>"},"metadata":{}},{"name":"stdout","text":"Stage 1: Fine-tuning classifier\nModel           : efficientnet\nWeights         : IMAGENET1K_V1\nFinetuned layers: classifier\n\n[Epoch 1, Step 1] train_loss: 2.4479\n[Epoch 1, Step 21] train_loss: 2.7361\n[Epoch 1, Step 41] train_loss: 2.7625\n[Epoch 1, Step 61] train_loss: 2.7209\n[Epoch 1, Step 81] train_loss: 2.7240\n[Epoch 1, Step 101] train_loss: 2.6927\n[Epoch 1, Step 121] train_loss: 2.6733\n[Epoch 1, Step 141] train_loss: 2.6529\n[Epoch 1, Step 161] train_loss: 2.6377\n[Epoch 1, Step 181] train_loss: 2.6286\n[Epoch 1, Step 201] train_loss: 2.6217\n[Epoch 1, Step 221] train_loss: 2.6174\n[Epoch 1, Step 241] train_loss: 2.6084\n[Epoch 1, Step 261] train_loss: 2.5951\n[Epoch 1, Step 281] train_loss: 2.5898\n[Epoch 1, Step 301] train_loss: 2.5709\n[Epoch 1, Step 321] train_loss: 2.5628\n[Epoch 1, Step 341] train_loss: 2.5622\n[Epoch 1, Step 361] train_loss: 2.5554\n[Epoch 1, Step 381] train_loss: 2.5416\n[Epoch 1, Step 389] train_loss: 2.5361\n[Epoch 1] Train Acc: 19.32%, Train Loss: 2.5361, Validation Acc: 29.11%, Validation Loss: 0.0768\n[Epoch 2, Step 1] train_loss: 2.3882\n[Epoch 2, Step 21] train_loss: 2.3348\n[Epoch 2, Step 41] train_loss: 2.4152\n[Epoch 2, Step 61] train_loss: 2.4161\n[Epoch 2, Step 81] train_loss: 2.3997\n[Epoch 2, Step 101] train_loss: 2.3784\n[Epoch 2, Step 121] train_loss: 2.3665\n[Epoch 2, Step 141] train_loss: 2.3683\n[Epoch 2, Step 161] train_loss: 2.3754\n[Epoch 2, Step 181] train_loss: 2.3598\n[Epoch 2, Step 201] train_loss: 2.3559\n[Epoch 2, Step 221] train_loss: 2.3654\n[Epoch 2, Step 241] train_loss: 2.3703\n[Epoch 2, Step 261] train_loss: 2.3676\n[Epoch 2, Step 281] train_loss: 2.3695\n[Epoch 2, Step 301] train_loss: 2.3671\n[Epoch 2, Step 321] train_loss: 2.3693\n[Epoch 2, Step 341] train_loss: 2.3652\n[Epoch 2, Step 361] train_loss: 2.3562\n[Epoch 2, Step 381] train_loss: 2.3616\n[Epoch 2, Step 389] train_loss: 2.3585\n[Epoch 2] Train Acc: 24.59%, Train Loss: 2.3585, Validation Acc: 29.11%, Validation Loss: 0.0763\n[Epoch 3, Step 1] train_loss: 2.4003\n[Epoch 3, Step 21] train_loss: 2.3332\n[Epoch 3, Step 41] train_loss: 2.2823\n[Epoch 3, Step 61] train_loss: 2.2899\n[Epoch 3, Step 81] train_loss: 2.3170\n[Epoch 3, Step 101] train_loss: 2.3112\n[Epoch 3, Step 121] train_loss: 2.3051\n[Epoch 3, Step 141] train_loss: 2.3065\n[Epoch 3, Step 161] train_loss: 2.3005\n[Epoch 3, Step 181] train_loss: 2.3061\n[Epoch 3, Step 201] train_loss: 2.3127\n[Epoch 3, Step 221] train_loss: 2.2997\n[Epoch 3, Step 241] train_loss: 2.3016\n[Epoch 3, Step 261] train_loss: 2.2985\n[Epoch 3, Step 281] train_loss: 2.3020\n[Epoch 3, Step 301] train_loss: 2.2964\n[Epoch 3, Step 321] train_loss: 2.2980\n[Epoch 3, Step 341] train_loss: 2.2990\n[Epoch 3, Step 361] train_loss: 2.2995\n[Epoch 3, Step 381] train_loss: 2.3001\n[Epoch 3, Step 389] train_loss: 2.2997\n[Epoch 3] Train Acc: 25.54%, Train Loss: 2.2997, Validation Acc: 31.46%, Validation Loss: 0.0731\n[Epoch 4, Step 1] train_loss: 2.0399\n[Epoch 4, Step 21] train_loss: 2.3273\n[Epoch 4, Step 41] train_loss: 2.3032\n[Epoch 4, Step 61] train_loss: 2.3193\n[Epoch 4, Step 81] train_loss: 2.2825\n[Epoch 4, Step 101] train_loss: 2.2850\n[Epoch 4, Step 121] train_loss: 2.2720\n[Epoch 4, Step 141] train_loss: 2.2815\n[Epoch 4, Step 161] train_loss: 2.2866\n[Epoch 4, Step 181] train_loss: 2.2868\n[Epoch 4, Step 201] train_loss: 2.2834\n[Epoch 4, Step 221] train_loss: 2.2913\n[Epoch 4, Step 241] train_loss: 2.2943\n[Epoch 4, Step 261] train_loss: 2.2944\n[Epoch 4, Step 281] train_loss: 2.2928\n[Epoch 4, Step 301] train_loss: 2.2847\n[Epoch 4, Step 321] train_loss: 2.2845\n[Epoch 4, Step 341] train_loss: 2.2782\n[Epoch 4, Step 361] train_loss: 2.2788\n[Epoch 4, Step 381] train_loss: 2.2777\n[Epoch 4, Step 389] train_loss: 2.2754\n[Epoch 4] Train Acc: 26.24%, Train Loss: 2.2754, Validation Acc: 31.97%, Validation Loss: 0.0724\n[Epoch 5, Step 1] train_loss: 2.2104\n[Epoch 5, Step 21] train_loss: 2.1925\n[Epoch 5, Step 41] train_loss: 2.2446\n[Epoch 5, Step 61] train_loss: 2.2155\n[Epoch 5, Step 81] train_loss: 2.2240\n[Epoch 5, Step 101] train_loss: 2.2156\n[Epoch 5, Step 121] train_loss: 2.2198\n[Epoch 5, Step 141] train_loss: 2.2376\n[Epoch 5, Step 161] train_loss: 2.2330\n[Epoch 5, Step 181] train_loss: 2.2327\n[Epoch 5, Step 201] train_loss: 2.2324\n[Epoch 5, Step 221] train_loss: 2.2313\n[Epoch 5, Step 241] train_loss: 2.2368\n[Epoch 5, Step 261] train_loss: 2.2374\n[Epoch 5, Step 281] train_loss: 2.2412\n[Epoch 5, Step 301] train_loss: 2.2375\n[Epoch 5, Step 321] train_loss: 2.2365\n[Epoch 5, Step 341] train_loss: 2.2395\n[Epoch 5, Step 361] train_loss: 2.2420\n[Epoch 5, Step 381] train_loss: 2.2399\n[Epoch 5, Step 389] train_loss: 2.2378\n[Epoch 5] Train Acc: 26.93%, Train Loss: 2.2378, Validation Acc: 32.42%, Validation Loss: 0.0699\n[Epoch 6, Step 1] train_loss: 2.5506\n[Epoch 6, Step 21] train_loss: 2.1963\n[Epoch 6, Step 41] train_loss: 2.2479\n[Epoch 6, Step 61] train_loss: 2.2362\n[Epoch 6, Step 81] train_loss: 2.1977\n[Epoch 6, Step 101] train_loss: 2.2027\n[Epoch 6, Step 121] train_loss: 2.2183\n[Epoch 6, Step 141] train_loss: 2.2283\n[Epoch 6, Step 161] train_loss: 2.2262\n[Epoch 6, Step 181] train_loss: 2.2380\n[Epoch 6, Step 201] train_loss: 2.2322\n[Epoch 6, Step 221] train_loss: 2.2379\n[Epoch 6, Step 241] train_loss: 2.2309\n[Epoch 6, Step 261] train_loss: 2.2341\n[Epoch 6, Step 281] train_loss: 2.2312\n[Epoch 6, Step 301] train_loss: 2.2342\n[Epoch 6, Step 321] train_loss: 2.2369\n[Epoch 6, Step 341] train_loss: 2.2330\n[Epoch 6, Step 361] train_loss: 2.2319\n[Epoch 6, Step 381] train_loss: 2.2344\n[Epoch 6, Step 389] train_loss: 2.2316\n[Epoch 6] Train Acc: 27.03%, Train Loss: 2.2316, Validation Acc: 32.33%, Validation Loss: 0.0727\nModel saved to: checkpoints/efficientnet_weights.pth\nHistory saved to: checkpoints/efficientnet_history.json\nStage 2: Fine-tuning all layers\n[Epoch 1, Step 1] train_loss: 2.2268\n[Epoch 1, Step 21] train_loss: 2.1930\n[Epoch 1, Step 41] train_loss: 2.1588\n[Epoch 1, Step 61] train_loss: 2.1753\n[Epoch 1, Step 81] train_loss: 2.1668\n[Epoch 1, Step 101] train_loss: 2.1687\n[Epoch 1, Step 121] train_loss: 2.1605\n[Epoch 1, Step 141] train_loss: 2.1606\n[Epoch 1, Step 161] train_loss: 2.1623\n[Epoch 1, Step 181] train_loss: 2.1654\n[Epoch 1, Step 201] train_loss: 2.1679\n[Epoch 1, Step 221] train_loss: 2.1629\n[Epoch 1, Step 241] train_loss: 2.1619\n[Epoch 1, Step 261] train_loss: 2.1528\n[Epoch 1, Step 281] train_loss: 2.1423\n[Epoch 1, Step 301] train_loss: 2.1350\n[Epoch 1, Step 321] train_loss: 2.1327\n[Epoch 1, Step 341] train_loss: 2.1249\n[Epoch 1, Step 361] train_loss: 2.1226\n[Epoch 1, Step 381] train_loss: 2.1266\n[Epoch 1, Step 389] train_loss: 2.1268\n[Epoch 1] Train Acc: 30.31%, Train Loss: 2.1268, Validation Acc: 37.31%, Validation Loss: 0.0665\n[Epoch 2, Step 1] train_loss: 1.9444\n[Epoch 2, Step 21] train_loss: 2.0100\n[Epoch 2, Step 41] train_loss: 1.9440\n[Epoch 2, Step 61] train_loss: 1.9183\n[Epoch 2, Step 81] train_loss: 1.9213\n[Epoch 2, Step 101] train_loss: 1.9247\n[Epoch 2, Step 121] train_loss: 1.9408\n[Epoch 2, Step 141] train_loss: 1.9462\n[Epoch 2, Step 161] train_loss: 1.9295\n[Epoch 2, Step 181] train_loss: 1.9257\n[Epoch 2, Step 201] train_loss: 1.9231\n[Epoch 2, Step 221] train_loss: 1.9284\n[Epoch 2, Step 241] train_loss: 1.9318\n[Epoch 2, Step 261] train_loss: 1.9348\n[Epoch 2, Step 281] train_loss: 1.9351\n[Epoch 2, Step 301] train_loss: 1.9421\n[Epoch 2, Step 321] train_loss: 1.9383\n[Epoch 2, Step 341] train_loss: 1.9466\n[Epoch 2, Step 361] train_loss: 1.9425\n[Epoch 2, Step 381] train_loss: 1.9411\n[Epoch 2, Step 389] train_loss: 1.9354\n[Epoch 2] Train Acc: 36.35%, Train Loss: 1.9354, Validation Acc: 42.06%, Validation Loss: 0.0588\n[Epoch 3, Step 1] train_loss: 1.8655\n[Epoch 3, Step 21] train_loss: 1.6868\n[Epoch 3, Step 41] train_loss: 1.7401\n[Epoch 3, Step 61] train_loss: 1.8181\n[Epoch 3, Step 81] train_loss: 1.8040\n[Epoch 3, Step 101] train_loss: 1.8041\n[Epoch 3, Step 121] train_loss: 1.8145\n[Epoch 3, Step 141] train_loss: 1.8302\n[Epoch 3, Step 161] train_loss: 1.8307\n[Epoch 3, Step 181] train_loss: 1.8258\n[Epoch 3, Step 201] train_loss: 1.8274\n[Epoch 3, Step 221] train_loss: 1.8213\n[Epoch 3, Step 241] train_loss: 1.8191\n[Epoch 3, Step 261] train_loss: 1.8160\n[Epoch 3, Step 281] train_loss: 1.8178\n[Epoch 3, Step 301] train_loss: 1.8198\n[Epoch 3, Step 321] train_loss: 1.8242\n[Epoch 3, Step 341] train_loss: 1.8233\n[Epoch 3, Step 361] train_loss: 1.8208\n[Epoch 3, Step 381] train_loss: 1.8236\n[Epoch 3, Step 389] train_loss: 1.8251\n[Epoch 3] Train Acc: 39.15%, Train Loss: 1.8251, Validation Acc: 43.03%, Validation Loss: 0.0563\n[Epoch 4, Step 1] train_loss: 2.3651\n[Epoch 4, Step 21] train_loss: 1.7186\n[Epoch 4, Step 41] train_loss: 1.6937\n[Epoch 4, Step 61] train_loss: 1.6624\n[Epoch 4, Step 81] train_loss: 1.6674\n[Epoch 4, Step 101] train_loss: 1.6866\n[Epoch 4, Step 121] train_loss: 1.6878\n[Epoch 4, Step 141] train_loss: 1.7001\n[Epoch 4, Step 161] train_loss: 1.6798\n[Epoch 4, Step 181] train_loss: 1.6808\n[Epoch 4, Step 201] train_loss: 1.6703\n[Epoch 4, Step 221] train_loss: 1.6624\n[Epoch 4, Step 241] train_loss: 1.6671\n[Epoch 4, Step 261] train_loss: 1.6564\n[Epoch 4, Step 281] train_loss: 1.6474\n[Epoch 4, Step 301] train_loss: 1.6413\n[Epoch 4, Step 321] train_loss: 1.6428\n[Epoch 4, Step 341] train_loss: 1.6411\n[Epoch 4, Step 361] train_loss: 1.6432\n[Epoch 4, Step 381] train_loss: 1.6461\n[Epoch 4, Step 389] train_loss: 1.6450\n[Epoch 4] Train Acc: 44.78%, Train Loss: 1.6450, Validation Acc: 47.56%, Validation Loss: 0.0560\n[Epoch 5, Step 1] train_loss: 1.4634\n[Epoch 5, Step 21] train_loss: 1.5000\n[Epoch 5, Step 41] train_loss: 1.4863\n[Epoch 5, Step 61] train_loss: 1.5208\n[Epoch 5, Step 81] train_loss: 1.5467\n[Epoch 5, Step 101] train_loss: 1.5802\n[Epoch 5, Step 121] train_loss: 1.5740\n[Epoch 5, Step 141] train_loss: 1.5595\n[Epoch 5, Step 161] train_loss: 1.5434\n[Epoch 5, Step 181] train_loss: 1.5334\n[Epoch 5, Step 201] train_loss: 1.5270\n[Epoch 5, Step 221] train_loss: 1.5341\n[Epoch 5, Step 241] train_loss: 1.5334\n[Epoch 5, Step 261] train_loss: 1.5242\n[Epoch 5, Step 281] train_loss: 1.5233\n[Epoch 5, Step 301] train_loss: 1.5179\n[Epoch 5, Step 321] train_loss: 1.5121\n[Epoch 5, Step 341] train_loss: 1.5173\n[Epoch 5, Step 361] train_loss: 1.5203\n[Epoch 5, Step 381] train_loss: 1.5246\n[Epoch 5, Step 389] train_loss: 1.5236\n[Epoch 5] Train Acc: 48.51%, Train Loss: 1.5236, Validation Acc: 49.39%, Validation Loss: 0.0508\n[Epoch 6, Step 1] train_loss: 1.1395\n[Epoch 6, Step 21] train_loss: 1.3765\n[Epoch 6, Step 41] train_loss: 1.3718\n[Epoch 6, Step 61] train_loss: 1.3749\n[Epoch 6, Step 81] train_loss: 1.3427\n[Epoch 6, Step 101] train_loss: 1.3433\n[Epoch 6, Step 121] train_loss: 1.3507\n[Epoch 6, Step 141] train_loss: 1.3627\n[Epoch 6, Step 161] train_loss: 1.3635\n[Epoch 6, Step 181] train_loss: 1.3730\n[Epoch 6, Step 201] train_loss: 1.3807\n[Epoch 6, Step 221] train_loss: 1.3797\n[Epoch 6, Step 241] train_loss: 1.3857\n[Epoch 6, Step 261] train_loss: 1.3884\n[Epoch 6, Step 281] train_loss: 1.4007\n[Epoch 6, Step 301] train_loss: 1.4029\n[Epoch 6, Step 321] train_loss: 1.4101\n[Epoch 6, Step 341] train_loss: 1.4034\n[Epoch 6, Step 361] train_loss: 1.4108\n[Epoch 6, Step 381] train_loss: 1.4142\n[Epoch 6, Step 389] train_loss: 1.4173\n[Epoch 6] Train Acc: 52.52%, Train Loss: 1.4173, Validation Acc: 51.22%, Validation Loss: 0.0488\n[Epoch 7, Step 1] train_loss: 1.1545\n[Epoch 7, Step 21] train_loss: 1.1913\n[Epoch 7, Step 41] train_loss: 1.2442\n[Epoch 7, Step 61] train_loss: 1.2571\n[Epoch 7, Step 81] train_loss: 1.2775\n[Epoch 7, Step 101] train_loss: 1.2912\n[Epoch 7, Step 121] train_loss: 1.2832\n[Epoch 7, Step 141] train_loss: 1.3005\n[Epoch 7, Step 161] train_loss: 1.2903\n[Epoch 7, Step 181] train_loss: 1.2898\n[Epoch 7, Step 201] train_loss: 1.2938\n[Epoch 7, Step 221] train_loss: 1.2810\n[Epoch 7, Step 241] train_loss: 1.2777\n[Epoch 7, Step 261] train_loss: 1.2724\n[Epoch 7, Step 281] train_loss: 1.2714\n[Epoch 7, Step 301] train_loss: 1.2780\n[Epoch 7, Step 321] train_loss: 1.2773\n[Epoch 7, Step 341] train_loss: 1.2759\n[Epoch 7, Step 361] train_loss: 1.2737\n[Epoch 7, Step 381] train_loss: 1.2795\n[Epoch 7, Step 389] train_loss: 1.2795\n[Epoch 7] Train Acc: 56.62%, Train Loss: 1.2795, Validation Acc: 53.50%, Validation Loss: 0.0489\n[Epoch 8, Step 1] train_loss: 1.3983\n[Epoch 8, Step 21] train_loss: 1.3217\n[Epoch 8, Step 41] train_loss: 1.2739\n[Epoch 8, Step 61] train_loss: 1.2232\n[Epoch 8, Step 81] train_loss: 1.2491\n[Epoch 8, Step 101] train_loss: 1.2380\n[Epoch 8, Step 121] train_loss: 1.2243\n[Epoch 8, Step 141] train_loss: 1.2346\n[Epoch 8, Step 161] train_loss: 1.2175\n[Epoch 8, Step 181] train_loss: 1.2087\n[Epoch 8, Step 201] train_loss: 1.2296\n[Epoch 8, Step 221] train_loss: 1.2230\n[Epoch 8, Step 241] train_loss: 1.2300\n[Epoch 8, Step 261] train_loss: 1.2303\n[Epoch 8, Step 281] train_loss: 1.2273\n[Epoch 8, Step 301] train_loss: 1.2218\n[Epoch 8, Step 321] train_loss: 1.2302\n[Epoch 8, Step 341] train_loss: 1.2244\n[Epoch 8, Step 361] train_loss: 1.2209\n[Epoch 8, Step 381] train_loss: 1.2101\n[Epoch 8, Step 389] train_loss: 1.2094\n[Epoch 8] Train Acc: 59.73%, Train Loss: 1.2094, Validation Acc: 55.17%, Validation Loss: 0.0459\n[Epoch 9, Step 1] train_loss: 0.5916\n[Epoch 9, Step 21] train_loss: 1.0560\n[Epoch 9, Step 41] train_loss: 1.0723\n[Epoch 9, Step 61] train_loss: 1.0818\n[Epoch 9, Step 81] train_loss: 1.1351\n[Epoch 9, Step 101] train_loss: 1.1151\n[Epoch 9, Step 121] train_loss: 1.0950\n[Epoch 9, Step 141] train_loss: 1.1022\n[Epoch 9, Step 161] train_loss: 1.1012\n[Epoch 9, Step 181] train_loss: 1.1080\n[Epoch 9, Step 201] train_loss: 1.1047\n[Epoch 9, Step 221] train_loss: 1.1055\n[Epoch 9, Step 241] train_loss: 1.0961\n[Epoch 9, Step 261] train_loss: 1.1049\n[Epoch 9, Step 281] train_loss: 1.1099\n[Epoch 9, Step 301] train_loss: 1.1129\n[Epoch 9, Step 321] train_loss: 1.1201\n[Epoch 9, Step 341] train_loss: 1.1260\n[Epoch 9, Step 361] train_loss: 1.1204\n[Epoch 9, Step 381] train_loss: 1.1168\n[Epoch 9, Step 389] train_loss: 1.1154\n[Epoch 9] Train Acc: 62.57%, Train Loss: 1.1154, Validation Acc: 55.49%, Validation Loss: 0.0454\n[Epoch 10, Step 1] train_loss: 0.6542\n[Epoch 10, Step 21] train_loss: 0.9288\n[Epoch 10, Step 41] train_loss: 0.9563\n[Epoch 10, Step 61] train_loss: 1.0436\n[Epoch 10, Step 81] train_loss: 1.0687\n[Epoch 10, Step 101] train_loss: 1.0611\n[Epoch 10, Step 121] train_loss: 1.0796\n[Epoch 10, Step 141] train_loss: 1.0874\n[Epoch 10, Step 161] train_loss: 1.0782\n[Epoch 10, Step 181] train_loss: 1.0876\n[Epoch 10, Step 201] train_loss: 1.0758\n[Epoch 10, Step 221] train_loss: 1.0818\n[Epoch 10, Step 241] train_loss: 1.0774\n[Epoch 10, Step 261] train_loss: 1.0756\n[Epoch 10, Step 281] train_loss: 1.0700\n[Epoch 10, Step 301] train_loss: 1.0641\n[Epoch 10, Step 321] train_loss: 1.0642\n[Epoch 10, Step 341] train_loss: 1.0614\n[Epoch 10, Step 361] train_loss: 1.0599\n[Epoch 10, Step 381] train_loss: 1.0648\n[Epoch 10, Step 389] train_loss: 1.0605\n[Epoch 10] Train Acc: 64.75%, Train Loss: 1.0605, Validation Acc: 56.43%, Validation Loss: 0.0428\n[Epoch 11, Step 1] train_loss: 0.6877\n[Epoch 11, Step 21] train_loss: 0.9859\n[Epoch 11, Step 41] train_loss: 0.9789\n[Epoch 11, Step 61] train_loss: 0.9759\n[Epoch 11, Step 81] train_loss: 0.9836\n[Epoch 11, Step 101] train_loss: 0.9932\n[Epoch 11, Step 121] train_loss: 0.9729\n[Epoch 11, Step 141] train_loss: 0.9720\n[Epoch 11, Step 161] train_loss: 0.9703\n[Epoch 11, Step 181] train_loss: 0.9660\n[Epoch 11, Step 201] train_loss: 0.9631\n[Epoch 11, Step 221] train_loss: 0.9739\n[Epoch 11, Step 241] train_loss: 0.9709\n[Epoch 11, Step 261] train_loss: 0.9595\n[Epoch 11, Step 281] train_loss: 0.9569\n[Epoch 11, Step 301] train_loss: 0.9702\n[Epoch 11, Step 321] train_loss: 0.9759\n[Epoch 11, Step 341] train_loss: 0.9763\n[Epoch 11, Step 361] train_loss: 0.9630\n[Epoch 11, Step 381] train_loss: 0.9798\n[Epoch 11, Step 389] train_loss: 0.9801\n[Epoch 11] Train Acc: 67.93%, Train Loss: 0.9801, Validation Acc: 57.62%, Validation Loss: 0.0437\n[Epoch 12, Step 1] train_loss: 0.7374\n[Epoch 12, Step 21] train_loss: 0.8953\n[Epoch 12, Step 41] train_loss: 0.9842\n[Epoch 12, Step 61] train_loss: 0.9248\n[Epoch 12, Step 81] train_loss: 0.8758\n[Epoch 12, Step 101] train_loss: 0.8904\n[Epoch 12, Step 121] train_loss: 0.9005\n[Epoch 12, Step 141] train_loss: 0.8994\n[Epoch 12, Step 161] train_loss: 0.8923\n[Epoch 12, Step 181] train_loss: 0.8892\n[Epoch 12, Step 201] train_loss: 0.8888\n[Epoch 12, Step 221] train_loss: 0.8979\n[Epoch 12, Step 241] train_loss: 0.8935\n[Epoch 12, Step 261] train_loss: 0.8949\n[Epoch 12, Step 281] train_loss: 0.8908\n[Epoch 12, Step 301] train_loss: 0.8960\n[Epoch 12, Step 321] train_loss: 0.8970\n[Epoch 12, Step 341] train_loss: 0.8901\n[Epoch 12, Step 361] train_loss: 0.8833\n[Epoch 12, Step 381] train_loss: 0.8818\n[Epoch 12, Step 389] train_loss: 0.8831\n[Epoch 12] Train Acc: 71.19%, Train Loss: 0.8831, Validation Acc: 58.74%, Validation Loss: 0.0467\n[Epoch 13, Step 1] train_loss: 0.7203\n[Epoch 13, Step 21] train_loss: 0.7482\n[Epoch 13, Step 41] train_loss: 0.8310\n[Epoch 13, Step 61] train_loss: 0.8404\n[Epoch 13, Step 81] train_loss: 0.8078\n[Epoch 13, Step 101] train_loss: 0.7898\n[Epoch 13, Step 121] train_loss: 0.7957\n[Epoch 13, Step 141] train_loss: 0.7977\n[Epoch 13, Step 161] train_loss: 0.7991\n[Epoch 13, Step 181] train_loss: 0.8229\n[Epoch 13, Step 201] train_loss: 0.8172\n[Epoch 13, Step 221] train_loss: 0.8138\n[Epoch 13, Step 241] train_loss: 0.8086\n[Epoch 13, Step 261] train_loss: 0.8306\n[Epoch 13, Step 281] train_loss: 0.8413\n[Epoch 13, Step 301] train_loss: 0.8500\n[Epoch 13, Step 321] train_loss: 0.8481\n[Epoch 13, Step 341] train_loss: 0.8426\n[Epoch 13, Step 361] train_loss: 0.8407\n[Epoch 13, Step 381] train_loss: 0.8421\n[Epoch 13, Step 389] train_loss: 0.8443\n[Epoch 13] Train Acc: 72.98%, Train Loss: 0.8443, Validation Acc: 58.77%, Validation Loss: 0.0457\n[Epoch 14, Step 1] train_loss: 0.5597\n[Epoch 14, Step 21] train_loss: 0.8232\n[Epoch 14, Step 41] train_loss: 0.7444\n[Epoch 14, Step 61] train_loss: 0.7620\n[Epoch 14, Step 81] train_loss: 0.7988\n[Epoch 14, Step 101] train_loss: 0.7997\n[Epoch 14, Step 121] train_loss: 0.8228\n[Epoch 14, Step 141] train_loss: 0.8258\n[Epoch 14, Step 161] train_loss: 0.8027\n[Epoch 14, Step 181] train_loss: 0.7952\n[Epoch 14, Step 201] train_loss: 0.7960\n[Epoch 14, Step 221] train_loss: 0.7932\n[Epoch 14, Step 241] train_loss: 0.7983\n[Epoch 14, Step 261] train_loss: 0.8159\n[Epoch 14, Step 281] train_loss: 0.8036\n[Epoch 14, Step 301] train_loss: 0.8113\n[Epoch 14, Step 321] train_loss: 0.8100\n[Epoch 14, Step 341] train_loss: 0.8181\n[Epoch 14, Step 361] train_loss: 0.8240\n[Epoch 14, Step 381] train_loss: 0.8235\n[Epoch 14, Step 389] train_loss: 0.8228\n[Epoch 14] Train Acc: 74.19%, Train Loss: 0.8228, Validation Acc: 59.54%, Validation Loss: 0.0424\n[Epoch 15, Step 1] train_loss: 0.7074\n[Epoch 15, Step 21] train_loss: 0.6385\n[Epoch 15, Step 41] train_loss: 0.7610\n[Epoch 15, Step 61] train_loss: 0.7386\n[Epoch 15, Step 81] train_loss: 0.7501\n[Epoch 15, Step 101] train_loss: 0.7313\n[Epoch 15, Step 121] train_loss: 0.7526\n[Epoch 15, Step 141] train_loss: 0.7681\n[Epoch 15, Step 161] train_loss: 0.7637\n[Epoch 15, Step 181] train_loss: 0.7452\n[Epoch 15, Step 201] train_loss: 0.7487\n[Epoch 15, Step 221] train_loss: 0.7647\n[Epoch 15, Step 241] train_loss: 0.7736\n[Epoch 15, Step 261] train_loss: 0.7661\n[Epoch 15, Step 281] train_loss: 0.7642\n[Epoch 15, Step 301] train_loss: 0.7781\n[Epoch 15, Step 321] train_loss: 0.7731\n[Epoch 15, Step 341] train_loss: 0.7809\n[Epoch 15, Step 361] train_loss: 0.7803\n[Epoch 15, Step 381] train_loss: 0.7787\n[Epoch 15, Step 389] train_loss: 0.7784\n[Epoch 15] Train Acc: 75.80%, Train Loss: 0.7784, Validation Acc: 60.76%, Validation Loss: 0.0427\n[Epoch 16, Step 1] train_loss: 1.3419\n[Epoch 16, Step 21] train_loss: 0.8470\n[Epoch 16, Step 41] train_loss: 0.7514\n[Epoch 16, Step 61] train_loss: 0.8176\n[Epoch 16, Step 81] train_loss: 0.8459\n[Epoch 16, Step 101] train_loss: 0.8353\n[Epoch 16, Step 121] train_loss: 0.8125\n[Epoch 16, Step 141] train_loss: 0.8015\n[Epoch 16, Step 161] train_loss: 0.7800\n[Epoch 16, Step 181] train_loss: 0.7926\n[Epoch 16, Step 201] train_loss: 0.7958\n[Epoch 16, Step 221] train_loss: 0.7879\n[Epoch 16, Step 241] train_loss: 0.7780\n[Epoch 16, Step 261] train_loss: 0.7660\n[Epoch 16, Step 281] train_loss: 0.7624\n[Epoch 16, Step 301] train_loss: 0.7616\n[Epoch 16, Step 321] train_loss: 0.7535\n[Epoch 16, Step 341] train_loss: 0.7666\n[Epoch 16, Step 361] train_loss: 0.7644\n[Epoch 16, Step 381] train_loss: 0.7669\n[Epoch 16, Step 389] train_loss: 0.7634\n[Epoch 16] Train Acc: 76.67%, Train Loss: 0.7634, Validation Acc: 60.93%, Validation Loss: 0.0402\n[Epoch 17, Step 1] train_loss: 0.4875\n[Epoch 17, Step 21] train_loss: 0.8035\n[Epoch 17, Step 41] train_loss: 0.7620\n[Epoch 17, Step 61] train_loss: 0.7990\n[Epoch 17, Step 81] train_loss: 0.7429\n[Epoch 17, Step 101] train_loss: 0.7263\n[Epoch 17, Step 121] train_loss: 0.7062\n[Epoch 17, Step 141] train_loss: 0.7212\n[Epoch 17, Step 161] train_loss: 0.7210\n[Epoch 17, Step 181] train_loss: 0.7156\n[Epoch 17, Step 201] train_loss: 0.7131\n[Epoch 17, Step 221] train_loss: 0.7237\n[Epoch 17, Step 241] train_loss: 0.7196\n[Epoch 17, Step 261] train_loss: 0.7248\n[Epoch 17, Step 281] train_loss: 0.7155\n[Epoch 17, Step 301] train_loss: 0.7232\n[Epoch 17, Step 321] train_loss: 0.7304\n[Epoch 17, Step 341] train_loss: 0.7294\n[Epoch 17, Step 361] train_loss: 0.7377\n[Epoch 17, Step 381] train_loss: 0.7357\n[Epoch 17, Step 389] train_loss: 0.7384\n[Epoch 17] Train Acc: 77.87%, Train Loss: 0.7384, Validation Acc: 61.44%, Validation Loss: 0.0434\n[Epoch 18, Step 1] train_loss: 0.6809\n[Epoch 18, Step 21] train_loss: 0.6736\n[Epoch 18, Step 41] train_loss: 0.6458\n[Epoch 18, Step 61] train_loss: 0.6656\n[Epoch 18, Step 81] train_loss: 0.6318\n[Epoch 18, Step 101] train_loss: 0.6591\n[Epoch 18, Step 121] train_loss: 0.6603\n[Epoch 18, Step 141] train_loss: 0.6601\n[Epoch 18, Step 161] train_loss: 0.6429\n[Epoch 18, Step 181] train_loss: 0.6529\n[Epoch 18, Step 201] train_loss: 0.6605\n[Epoch 18, Step 221] train_loss: 0.6868\n[Epoch 18, Step 241] train_loss: 0.6909\n[Epoch 18, Step 261] train_loss: 0.6816\n[Epoch 18, Step 281] train_loss: 0.6772\n[Epoch 18, Step 301] train_loss: 0.6649\n[Epoch 18, Step 321] train_loss: 0.6827\n[Epoch 18, Step 341] train_loss: 0.6879\n[Epoch 18, Step 361] train_loss: 0.6848\n[Epoch 18, Step 381] train_loss: 0.6933\n[Epoch 18, Step 389] train_loss: 0.6952\n[Epoch 18] Train Acc: 79.62%, Train Loss: 0.6952, Validation Acc: 61.50%, Validation Loss: 0.0433\n[Epoch 19, Step 1] train_loss: 0.3163\n[Epoch 19, Step 21] train_loss: 0.6089\n[Epoch 19, Step 41] train_loss: 0.5743\n[Epoch 19, Step 61] train_loss: 0.6298\n[Epoch 19, Step 81] train_loss: 0.6838\n[Epoch 19, Step 101] train_loss: 0.6996\n[Epoch 19, Step 121] train_loss: 0.7090\n[Epoch 19, Step 141] train_loss: 0.6833\n[Epoch 19, Step 161] train_loss: 0.6883\n[Epoch 19, Step 181] train_loss: 0.6941\n[Epoch 19, Step 201] train_loss: 0.7038\n[Epoch 19, Step 221] train_loss: 0.6977\n[Epoch 19, Step 241] train_loss: 0.6969\n[Epoch 19, Step 261] train_loss: 0.6884\n[Epoch 19, Step 281] train_loss: 0.6869\n[Epoch 19, Step 301] train_loss: 0.6746\n[Epoch 19, Step 321] train_loss: 0.6833\n[Epoch 19, Step 341] train_loss: 0.6771\n[Epoch 19, Step 361] train_loss: 0.6814\n[Epoch 19, Step 381] train_loss: 0.6790\n[Epoch 19, Step 389] train_loss: 0.6779\n[Epoch 19] Train Acc: 80.53%, Train Loss: 0.6779, Validation Acc: 61.83%, Validation Loss: 0.0443\n[Epoch 20, Step 1] train_loss: 0.8966\n[Epoch 20, Step 21] train_loss: 0.7439\n[Epoch 20, Step 41] train_loss: 0.6333\n[Epoch 20, Step 61] train_loss: 0.5822\n[Epoch 20, Step 81] train_loss: 0.5858\n[Epoch 20, Step 101] train_loss: 0.5808\n[Epoch 20, Step 121] train_loss: 0.5769\n[Epoch 20, Step 141] train_loss: 0.5730\n[Epoch 20, Step 161] train_loss: 0.5887\n[Epoch 20, Step 181] train_loss: 0.5894\n[Epoch 20, Step 201] train_loss: 0.6277\n[Epoch 20, Step 221] train_loss: 0.6529\n[Epoch 20, Step 241] train_loss: 0.6519\n[Epoch 20, Step 261] train_loss: 0.6479\n[Epoch 20, Step 281] train_loss: 0.6434\n[Epoch 20, Step 301] train_loss: 0.6358\n[Epoch 20, Step 321] train_loss: 0.6362\n[Epoch 20, Step 341] train_loss: 0.6350\n[Epoch 20, Step 361] train_loss: 0.6392\n[Epoch 20, Step 381] train_loss: 0.6502\n[Epoch 20, Step 389] train_loss: 0.6555\n[Epoch 20] Train Acc: 81.53%, Train Loss: 0.6555, Validation Acc: 62.21%, Validation Loss: 0.0402\n[Epoch 21, Step 1] train_loss: 0.2390\n[Epoch 21, Step 21] train_loss: 0.6374\n[Epoch 21, Step 41] train_loss: 0.5820\n[Epoch 21, Step 61] train_loss: 0.5530\n[Epoch 21, Step 81] train_loss: 0.5674\n[Epoch 21, Step 101] train_loss: 0.5862\n[Epoch 21, Step 121] train_loss: 0.5864\n[Epoch 21, Step 141] train_loss: 0.5985\n[Epoch 21, Step 161] train_loss: 0.5937\n[Epoch 21, Step 181] train_loss: 0.5972\n[Epoch 21, Step 201] train_loss: 0.6058\n[Epoch 21, Step 221] train_loss: 0.6066\n[Epoch 21, Step 241] train_loss: 0.6096\n[Epoch 21, Step 261] train_loss: 0.6241\n[Epoch 21, Step 281] train_loss: 0.6071\n[Epoch 21, Step 301] train_loss: 0.6033\n[Epoch 21, Step 321] train_loss: 0.6157\n[Epoch 21, Step 341] train_loss: 0.6228\n[Epoch 21, Step 361] train_loss: 0.6192\n[Epoch 21, Step 381] train_loss: 0.6146\n[Epoch 21, Step 389] train_loss: 0.6124\n[Epoch 21] Train Acc: 82.99%, Train Loss: 0.6124, Validation Acc: 62.02%, Validation Loss: 0.0430\n[Epoch 22, Step 1] train_loss: 0.4073\n[Epoch 22, Step 21] train_loss: 0.7253\n[Epoch 22, Step 41] train_loss: 0.6329\n[Epoch 22, Step 61] train_loss: 0.5718\n[Epoch 22, Step 81] train_loss: 0.6046\n[Epoch 22, Step 101] train_loss: 0.5947\n[Epoch 22, Step 121] train_loss: 0.5899\n[Epoch 22, Step 141] train_loss: 0.5610\n[Epoch 22, Step 161] train_loss: 0.5707\n[Epoch 22, Step 181] train_loss: 0.5711\n[Epoch 22, Step 201] train_loss: 0.5677\n[Epoch 22, Step 221] train_loss: 0.5639\n[Epoch 22, Step 241] train_loss: 0.5605\n[Epoch 22, Step 261] train_loss: 0.5509\n[Epoch 22, Step 281] train_loss: 0.5673\n[Epoch 22, Step 301] train_loss: 0.5687\n[Epoch 22, Step 321] train_loss: 0.5766\n[Epoch 22, Step 341] train_loss: 0.5825\n[Epoch 22, Step 361] train_loss: 0.5826\n[Epoch 22, Step 381] train_loss: 0.5817\n[Epoch 22, Step 389] train_loss: 0.5843\n[Epoch 22] Train Acc: 84.59%, Train Loss: 0.5843, Validation Acc: 62.31%, Validation Loss: 0.0444\n[Epoch 23, Step 1] train_loss: 0.2430\n[Epoch 23, Step 21] train_loss: 0.5710\n[Epoch 23, Step 41] train_loss: 0.5757\n[Epoch 23, Step 61] train_loss: 0.5496\n[Epoch 23, Step 81] train_loss: 0.6620\n[Epoch 23, Step 101] train_loss: 0.6185\n[Epoch 23, Step 121] train_loss: 0.6279\n[Epoch 23, Step 141] train_loss: 0.6086\n[Epoch 23, Step 161] train_loss: 0.5878\n[Epoch 23, Step 181] train_loss: 0.5715\n[Epoch 23, Step 201] train_loss: 0.5733\n[Epoch 23, Step 221] train_loss: 0.6025\n[Epoch 23, Step 241] train_loss: 0.6025\n[Epoch 23, Step 261] train_loss: 0.5929\n[Epoch 23, Step 281] train_loss: 0.5999\n[Epoch 23, Step 301] train_loss: 0.5953\n[Epoch 23, Step 321] train_loss: 0.6008\n[Epoch 23, Step 341] train_loss: 0.5969\n[Epoch 23, Step 361] train_loss: 0.5970\n[Epoch 23, Step 381] train_loss: 0.5958\n[Epoch 23, Step 389] train_loss: 0.5957\n[Epoch 23] Train Acc: 83.95%, Train Loss: 0.5957, Validation Acc: 62.63%, Validation Loss: 0.0437\n[Epoch 24, Step 1] train_loss: 0.2500\n[Epoch 24, Step 21] train_loss: 0.6718\n[Epoch 24, Step 41] train_loss: 0.6186\n[Epoch 24, Step 61] train_loss: 0.5601\n[Epoch 24, Step 81] train_loss: 0.5428\n[Epoch 24, Step 101] train_loss: 0.5264\n[Epoch 24, Step 121] train_loss: 0.5333\n[Epoch 24, Step 141] train_loss: 0.5163\n[Epoch 24, Step 161] train_loss: 0.5127\n[Epoch 24, Step 181] train_loss: 0.5159\n[Epoch 24, Step 201] train_loss: 0.5277\n[Epoch 24, Step 221] train_loss: 0.5236\n[Epoch 24, Step 241] train_loss: 0.5159\n[Epoch 24, Step 261] train_loss: 0.5291\n[Epoch 24, Step 281] train_loss: 0.5314\n[Epoch 24, Step 301] train_loss: 0.5453\n[Epoch 24, Step 321] train_loss: 0.5492\n[Epoch 24, Step 341] train_loss: 0.5551\n[Epoch 24, Step 361] train_loss: 0.5481\n[Epoch 24, Step 381] train_loss: 0.5457\n[Epoch 24, Step 389] train_loss: 0.5430\n[Epoch 24] Train Acc: 86.00%, Train Loss: 0.5430, Validation Acc: 62.98%, Validation Loss: 0.0418\nModel saved to: checkpoints/efficientnet_weights.pth\nHistory saved to: checkpoints/efficientnet_history.json\nEfficientNet Test Loss: 0.039, Test Accuracy: 64.02%\nModel saved.\n","output_type":"stream"}],"execution_count":28},{"id":"590b2f72-4436-4230-a90b-577c4c22f323","cell_type":"markdown","source":"# Evaluation","metadata":{}},{"id":"5d8f9f80-a307-4acc-acff-ffd40241bcab","cell_type":"code","source":"#### report\nfrom sklearn.metrics import classification_report, precision_recall_curve, average_precision_score, confusion_matrix\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport os\nfrom typing import Tuple, Dict\nimport torchvision.models as models\n\n\ndef load_model(model_name: str, num_classes: int, weights_path: str, device: str) -> nn.Module:\n    if model_name == 'densenet':\n        model = models.densenet121(weights=None)\n        in_features = model.classifier.in_features\n        model.classifier = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n    elif model_name == 'mobilenet':\n        model = models.mobilenet_v2(weights=None)\n        in_features = model.classifier[1].in_features\n        model.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(in_features, num_classes)\n        )\n    elif model_name == 'efficientnet':\n        model = models.efficientnet_b3(weights=None)\n        in_features = model.classifier[1].in_features\n        model.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(in_features, num_classes)\n        )\n    else:\n        raise ValueError(\"Unsupported model type\")\n\n    model.load_state_dict(torch.load(weights_path, map_location=device, weights_only=True))\n\n    return model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:33:00.790644Z","iopub.execute_input":"2025-05-04T08:33:00.790914Z","iopub.status.idle":"2025-05-04T08:33:03.211808Z","shell.execute_reply.started":"2025-05-04T08:33:00.790890Z","shell.execute_reply":"2025-05-04T08:33:03.211268Z"}},"outputs":[],"execution_count":29},{"id":"946fc88c-37f3-4f52-b666-d62c9bc7cf94","cell_type":"code","source":"#model_densenet = load_model(model_name='densenet', num_classes= num_classes, weights_path='checkpoints/model_densenet_weights.pth', device=device)\nmodel_efficientnet = load_model(model_name='efficientnet', num_classes=num_classes, weights_path='efficientnet_b3_all_lr0.0001.safetensors', device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:33:03.212479Z","iopub.execute_input":"2025-05-04T08:33:03.212779Z","iopub.status.idle":"2025-05-04T08:33:03.473376Z","shell.execute_reply.started":"2025-05-04T08:33:03.212761Z","shell.execute_reply":"2025-05-04T08:33:03.471851Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_153/1683556398.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model_densenet = load_model(model_name='densenet', num_classes= num_classes, weights_path='checkpoints/model_densenet_weights.pth', device=device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_efficientnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'efficientnet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'efficientnet_b3_all_lr0.0001.safetensors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_153/1264750377.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_name, num_classes, weights_path, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsupported model type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 )\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m         return _legacy_load(\n\u001b[1;32m   1385\u001b[0m             \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\nPlease file an issue with the following so that we can make `weights_only=True` compatible with your use case: WeightsUnpickler error: Unsupported operand 112\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."],"ename":"UnpicklingError","evalue":"Weights only load failed. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\nPlease file an issue with the following so that we can make `weights_only=True` compatible with your use case: WeightsUnpickler error: Unsupported operand 112\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.","output_type":"error"}],"execution_count":30},{"id":"5b5d2b3e-997d-4eff-8a0f-591ea1374214","cell_type":"code","source":"def load_history_json(history_path: str) -> Dict:\n    with open(history_path, 'r') as f:\n        history = json.load(f)\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:33:03.473736Z","iopub.status.idle":"2025-05-04T08:33:03.473951Z","shell.execute_reply.started":"2025-05-04T08:33:03.473844Z","shell.execute_reply":"2025-05-04T08:33:03.473854Z"}},"outputs":[],"execution_count":null},{"id":"f8e68bda-5b42-48e8-ab71-0f8c9ef1d0e3","cell_type":"code","source":"history = load_history_json('checkpoints/model_densenet_history.json')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:33:03.474849Z","iopub.status.idle":"2025-05-04T08:33:03.475059Z","shell.execute_reply.started":"2025-05-04T08:33:03.474958Z","shell.execute_reply":"2025-05-04T08:33:03.474968Z"}},"outputs":[],"execution_count":null},{"id":"ae83ca52-35f7-4ecc-911a-20fd8dc43f19","cell_type":"code","source":"test_loss, test_acc = evaluate(model_densenet, test_loader, loss_function, device)\nprint(f\"Loaded Model Test Loss: {test_loss:.3f}, Test Accuracy: {test_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:33:03.475976Z","iopub.status.idle":"2025-05-04T08:33:03.476308Z","shell.execute_reply.started":"2025-05-04T08:33:03.476121Z","shell.execute_reply":"2025-05-04T08:33:03.476154Z"}},"outputs":[],"execution_count":null},{"id":"53e9fbd9-c652-4675-b9ed-9ef5c0d61db2","cell_type":"code","source":"\ndef get_classification_report(model, test_loader, class_names, device):\n    model.eval()\n    y_true, y_pred = [], []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n    \n    report = classification_report(y_true, y_pred, target_names=class_names, digits=3)\n    return report\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:33:03.478082Z","iopub.status.idle":"2025-05-04T08:33:03.478466Z","shell.execute_reply.started":"2025-05-04T08:33:03.478291Z","shell.execute_reply":"2025-05-04T08:33:03.478309Z"}},"outputs":[],"execution_count":null},{"id":"fceece8d-2dec-4fb2-a465-9bef664eeab2","cell_type":"code","source":"report = get_classification_report(model_efficientnet, test_loader, class_names, device)\nprint(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:33:03.479426Z","iopub.status.idle":"2025-05-04T08:33:03.479730Z","shell.execute_reply.started":"2025-05-04T08:33:03.479569Z","shell.execute_reply":"2025-05-04T08:33:03.479584Z"}},"outputs":[],"execution_count":null},{"id":"32b19b66-9c6c-45b3-bfc5-807ab7133797","cell_type":"code","source":"\ndef plot_weighted_avg_precision_recall_curve(model, test_loader, class_names, device):\n    model.eval()\n    y_true = []\n    y_scores = []\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n            y_scores.extend(probs)\n            y_true.extend(labels.cpu().numpy())\n\n    y_scores = np.array(y_scores)\n    y_true_bin = label_binarize(y_true, classes=list(range(len(class_names))))\n\n    precision, recall, _ = precision_recall_curve(y_true_bin.ravel(), y_scores.ravel())\n    ap = average_precision_score(y_true_bin, y_scores, average='weighted')\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(recall, precision, label=f'Micro-Averaged AP = {ap:.2f}')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Weighted-Averaged Precision-Recall Curve')\n    plt.legend(loc='best')\n    plt.grid(True)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:33:03.480592Z","iopub.status.idle":"2025-05-04T08:33:03.480913Z","shell.execute_reply.started":"2025-05-04T08:33:03.480754Z","shell.execute_reply":"2025-05-04T08:33:03.480769Z"}},"outputs":[],"execution_count":null},{"id":"ff06e97b-ac0f-478f-ae2a-b9bbec1f0b56","cell_type":"code","source":"plot_weighted_avg_precision_recall_curve(model_densenet, test_loader, class_names, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:33:03.481690Z","iopub.status.idle":"2025-05-04T08:33:03.482018Z","shell.execute_reply.started":"2025-05-04T08:33:03.481860Z","shell.execute_reply":"2025-05-04T08:33:03.481875Z"}},"outputs":[],"execution_count":null},{"id":"765a941d-716d-443d-be5a-392ca929c1bc","cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndef plot_confusion_matrix(model, test_loader, class_names, device):\n    model.eval()\n    y_true, y_pred = [], []\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n    \n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:33:03.482866Z","iopub.status.idle":"2025-05-04T08:33:03.483068Z","shell.execute_reply.started":"2025-05-04T08:33:03.482971Z","shell.execute_reply":"2025-05-04T08:33:03.482981Z"}},"outputs":[],"execution_count":null},{"id":"b5a30fa5-c9df-482c-9bde-19f8a8087770","cell_type":"code","source":"plot_confusion_matrix(model_densenet, test_loader, class_names, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:33:03.484227Z","iopub.status.idle":"2025-05-04T08:33:03.484506Z","shell.execute_reply.started":"2025-05-04T08:33:03.484371Z","shell.execute_reply":"2025-05-04T08:33:03.484385Z"}},"outputs":[],"execution_count":null},{"id":"cb738e5c-dc1c-4c91-82bd-52a5750b5239","cell_type":"code","source":"#!/usr/bin/env python3\n# inference_mobilenetv2.py - Test script for MobileNetV2 model\n\nimport sys\nimport torch\nimport argparse\nfrom torchvision.models import MobileNet_V2_Weights\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Import the build_model function from your module\n# Assuming the build_model function is in a file called models.py\n# If it's in a different file, change the import accordingly\nfrom models import build_model\n\n# ——— CONFIGURATION ———\nMODEL_PATH = \"mbnet_dermnet_v2.pt\"\nNUM_CLASSES = 23\nCLASS_NAMES = [\n    \"Light Diseases and Disorders of Pigmentation\",\n    \"Lupus and other Connective Tissue diseases\",\n    \"Acne and Rosacea Photos\",\n    \"Systemic Disease\",\n    \"Poison Ivy Photos and other Contact Dermatitis\",\n    \"Vascular Tumors\",\n    \"Urticaria Hives\",\n    \"Atopic Dermatitis Photos\",\n    \"Bullous Disease Photos\",\n    \"Hair Loss Photos Alopecia and other Hair Diseases\",\n    \"Tinea Ringworm Candidiasis and other Fungal Infections\",\n    \"Psoriasis pictures Lichen Planus and related diseases\",\n    \"Melanoma Skin Cancer Nevi and Moles\",\n    \"Nail Fungus and other Nail Disease\",\n    \"Scabies Lyme Disease and other Infestations and Bites\",\n    \"Eczema Photos\",\n    \"Exanthems and Drug Eruptions\",\n    \"Herpes HPV and other STDs Photos\",\n    \"Seborrheic Keratoses and other Benign Tumors\",\n    \"Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions\",\n    \"Vasculitis Photos\",\n    \"Cellulitis Impetigo and other Bacterial Infections\",\n    \"Warts Molluscum and other Viral Infections\"\n]\n\nIMAGE_SIZE = 224  # match your training size\n\ndef load_model(model_path, device='cpu'):\n    \"\"\"\n    Load a saved MobileNetV2 model\n    \"\"\"\n    # Define model configuration\n    config = {\n        'weights': None,  # We'll load from saved weights, not pretrained\n        'lr': 1e-4,\n        'weight_decay': 1e-4,\n        'finetuned_layers': 'all'\n    }\n    \n    # Build the model architecture using your function\n    model = build_model('mobilenet', NUM_CLASSES, config, device=device, return_optimizer=False)\n    \n    # Load the saved weights\n    checkpoint = torch.load(model_path, map_location=device)\n    \n    # Handle different saving formats\n    if isinstance(checkpoint, dict) and \"state_dict\" in checkpoint:\n        model.load_state_dict(checkpoint[\"state_dict\"])\n    elif isinstance(checkpoint, dict) and \"model\" in checkpoint:\n        model.load_state_dict(checkpoint[\"model\"])\n    else:\n        try:\n            model.load_state_dict(checkpoint)\n        except RuntimeError as e:\n            print(f\"Error loading model: {e}\")\n            # If it's the entire model object\n            if isinstance(checkpoint, torch.nn.Module):\n                model = checkpoint\n    \n    model.eval()\n    return model\n\ndef get_transforms():\n    \"\"\"\n    Create image preprocessing transformations\n    \"\"\"\n    return transforms.Compose([\n        transforms.Resize(int(IMAGE_SIZE * 1.14)),\n        transforms.CenterCrop(IMAGE_SIZE),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n        ),\n    ])\n\ndef predict(model, image_path, transform, topk=5, device='cpu'):\n    \"\"\"\n    Make predictions on an image\n    \"\"\"\n    # Open and preprocess the image\n    img = Image.open(image_path).convert(\"RGB\")\n    x = transform(img).unsqueeze(0).to(device)\n    \n    # Make prediction\n    with torch.no_grad():\n        logits = model(x)\n        probs = torch.nn.functional.softmax(logits[0], dim=0)\n    \n    # Get top k predictions\n    top_probs, top_idxs = probs.topk(topk)\n    \n    print(f\"\\nTop {topk} predictions for {image_path}:\")\n    for p, idx in zip(top_probs, top_idxs):\n        print(f\"  {CLASS_NAMES[idx]} — {p.item():.4f}\")\n    \n    return top_probs, top_idxs\n\ndef main():\n    parser = argparse.ArgumentParser(description='Inference with MobileNetV2 model')\n    parser.add_argument('image_path', type=str, help='Path to input image')\n    parser.add_argument('--model', type=str, default=MODEL_PATH, help='Path to model weights')\n    parser.add_argument('--topk', type=int, default=5, help='Show top k predictions')\n    parser.add_argument('--device', type=str, default='cpu', help='Device to run inference on (cpu/cuda)')\n    args = parser.parse_args()\n    \n    # Check if CUDA is available\n    if args.device == 'cuda' and not torch.cuda.is_available():\n        print(\"CUDA is not available. Using CPU instead.\")\n        args.device = 'cpu'\n    \n    print(f\"Loading model from {args.model}...\")\n    model = load_model(args.model, device=args.device)\n    \n    transform = get_transforms()\n    \n    print(f\"Running inference on {args.image_path}...\")\n    predict(model, args.image_path, transform, args.topk, device=args.device)\n\nif _name_ == \"_main_\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:33:03.485578Z","iopub.status.idle":"2025-05-04T08:33:03.485780Z","shell.execute_reply.started":"2025-05-04T08:33:03.485683Z","shell.execute_reply":"2025-05-04T08:33:03.485692Z"}},"outputs":[],"execution_count":null},{"id":"21b0451b-8f88-4380-bcd1-c6d56687718e","cell_type":"code","source":"# models.py - Contains the model building functions\n\nfrom torchvision.models import (\n    densenet121, mobilenet_v2, efficientnet_b3, DenseNet121_Weights, MobileNet_V2_Weights, EfficientNet_B3_Weights\n)\nimport torch.nn as nn\nimport torch.optim as optim\n\ndef build_model(model_name, num_classes, config, device='cuda', return_optimizer=True):\n    \"\"\"\n    Build and configure a neural network model.\n    \n    Args:\n        model_name (str): Name of the model ('densenet', 'mobilenet', or 'efficientnet')\n        num_classes (int): Number of output classes\n        config (dict): Configuration with weights, learning rate, etc.\n        device (str): Device to put the model on ('cuda' or 'cpu')\n        return_optimizer (bool): Whether to return optimizer along with the model\n        \n    Returns:\n        model or (model, optimizer) depending on return_optimizer\n    \"\"\"\n    # Select model\n    if model_name == 'densenet':\n        model = densenet121(weights=config['weights']).to(device)\n        in_features = model.classifier.in_features\n        model.classifier = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        ).to(device)\n    elif model_name == 'mobilenet':\n        model = mobilenet_v2(weights=config['weights']).to(device)\n        in_features = model.classifier[1].in_features\n        model.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(in_features, num_classes)\n        ).to(device)\n    elif model_name == 'efficientnet':\n        model = efficientnet_b3(weights=config['weights']).to(device)\n        in_features = model.classifier[1].in_features\n        model.classifier = nn.Sequential(\n            nn.Dropout(0.65),\n            nn.Linear(in_features, num_classes)\n        ).to(device)\n    else:\n        raise ValueError(f\"Unsupported model name: {model_name}\")\n        \n    # Finetuning strategy\n    finetuned_layers = config.get('finetuned_layers', 'all')\n    if finetuned_layers == 'classifier':\n        for param in model.parameters():\n            param.requires_grad = False\n        for param in model.classifier.parameters():\n            param.requires_grad = True\n    elif finetuned_layers == 'features_last' and model_name == 'efficientnet':\n        for param in model.parameters():\n            param.requires_grad = False\n        for param in model.features[-1].parameters():\n            param.requires_grad = True\n        for param in model.classifier.parameters():\n            param.requires_grad = True\n    elif finetuned_layers == 'all':\n        for param in model.parameters():\n            param.requires_grad = True\n    else:\n        for name, param in model.named_parameters():\n            if not any(name.startswith(layer) for layer in config['finetuned_layers']):\n                param.requires_grad = False\n                \n    # Print model info\n    print(f\"Model           : {model_name}\")\n    print(f\"Weights         : {config['weights']}\")\n    print(f\"Finetuned layers: {finetuned_layers}\\n\")\n    \n    # Optimizer\n    if not return_optimizer:\n        return model\n        \n    optimizer = optim.AdamW(\n        filter(lambda p: p.requires_grad, model.parameters()),\n        lr=config['lr'],\n        weight_decay=config.get('weight_decay', 1e-4)\n    )\n    \n    return model, optimizer\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}